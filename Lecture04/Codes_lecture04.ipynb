{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lecutre 4: support material**\n",
    "\n",
    "The codes in the cells below are examples presented in the slides of Lecture 4. Use the lecture slides as reference for comments and other informations. \n",
    "\n",
    "##### Things to consider:\n",
    "If an error occures while executing Cell 1 and the error is associated to pyspark (i.e., not found):\n",
    "\n",
    "###### Q1: Are you missing pyspark?\n",
    "\n",
    "$ python <br>\n",
    "import pyspark <br>\n",
    "Traceback (most recent call last): <br>\n",
    "  File \"<stdin>\", line 1, in <module> <br>\n",
    "ModuleNotFoundError: No module named 'pyspark' <br>\n",
    "exit() <br>\n",
    "\n",
    "Install pyspark (on Linux and Mac, and in Anaconda command prompt in windows): <br>\n",
    "$ pip install pyspark\n",
    "\n",
    "Now follow the steps in the Q2\n",
    "\n",
    "###### Q2: Is your setting properly installed? \n",
    "\n",
    "Test pyspark on python (not Jupyter): <br>\n",
    "$ python <br>\n",
    " from pyspark import SparkContext <br>\n",
    " sc = SparkContext.getOrCreate()  <br>\n",
    "\n",
    "Find pyspark: <br>\n",
    "$ which pyspark <br>\n",
    "/Users/taufer/opt/anaconda3/bin/pyspark\n",
    "\n",
    "Set your env: <br>\n",
    "$ emacs .bash_profile <br>\n",
    "\n",
    "Add these two new paths <br>\n",
    "\n",
    "export PATH=\"/Users/taufer/opt/anaconda3/bin:$PATH\" <br>\n",
    "export PYTHONPATH=\"/Users/taufer/opt/anaconda3/bin\" <br>\n",
    "\n",
    "Make sure they are are seen by your OS, if not: <br>\n",
    "\n",
    "$ source .bash_profile <br>\n",
    "\n",
    "###### Q3: Is your Python version too new (e.g., 3.8)?\n",
    "If you have python version > 3.7 do the following\n",
    "\n",
    "Install python 3.7 <br>\n",
    "Install virtualenv <br>\n",
    "\n",
    "Create a new virtual environment using python 3.7 <br>\n",
    "$ virtualenv --python=/path/to/python37 ./spark_env <br>\n",
    "\n",
    "Activate environment <br>\n",
    "$ source ./spark_env/bin/activate <br>\n",
    "\n",
    "Install pyspark <br>\n",
    "$ pip install pyspark <br>\n",
    "\n",
    "Install jupyter <br>\n",
    "$ pip install jupyter <br>\n",
    "\n",
    "Run jupyter <br>\n",
    "$ ./spark_env/bin/jupyter notebook <br>\n",
    "\n",
    "Deactivate environment <br>\n",
    "$ deactivate <br>\n",
    "\n",
    "##### Q4: Are you getting Java related errors?\n",
    "If you have Java errors make sure you are running Java 8. <br>\n",
    "\n",
    "Spark does not support newer versions <br>\n",
    "\n",
    "$ java --version <br>\n",
    "\n",
    "##### Q5: (Windows) Is Java installed, and you still have a \"Java gateway\" error?\n",
    "\n",
    "Restart Anaconda and Jupyter, especially if you installed Java with Anaconda open.\n",
    "\n",
    "If you still have the error, check if your personal User folder in Windows has a space in it. If so, you'll need to reinstall Anaconda for all users instead of for just you. \n",
    "\n",
    "##### Q6: (Windows) Is winutils not found?\n",
    "\n",
    "This happened because some files were not included in your Spark installation by default. Do this to get them: \n",
    "\n",
    "Clone this repository: https://github.com/cdarlint/winutils<br>\n",
    "\n",
    "Open your advanced settings and environment variables. <br>\n",
    "\n",
    "Click the Environment Variables button, near the bottom of that dialog. <br>\n",
    "\n",
    "In the user variables, section, click New... <br>\n",
    "\n",
    "Name the variable HADOOP_HOME, use the Browse Directory button, and choose the folder in the repository you cloned with the same version as your pyspark. For example, if you have Pyspark 3.0.1, select hadoop-3.0.1. Do not select hadoop-x.x.x/bin. The path will be looking for a folder that has bin within it. <br>\n",
    "\n",
    "Click ok, then select the User variable called \"Path,\" and choose Edit... <br>\n",
    "\n",
    "Click New in this new dialog box. Enter %HADOOP_HOME%\\ in the new space, then click all the 'Ok' you need to to get out of this. <br>\n",
    "\n",
    "Restart Anaconda and Jupyter, then run again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyspark\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "sc.setLogLevel(\"WARN\")\n",
    "# Load list of words\n",
    "lines = sc.textFile('FoxInSocks.txt')\n",
    "# Count the number of items in this RDD\n",
    "print(lines.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First item in this RDD, i.e. first line of FoxInSocks.txt\n",
    "print(lines.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc = SparkContext.getOrCreate()\n",
    "numbers = sc.parallelize([1, 2, 3, 3], 4)\n",
    "squared = numbers.map(lambda x: x * x)\n",
    "squared.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc = SparkContext.getOrCreate()\n",
    "\n",
    "lines = sc.parallelize([\"hello world\", \"hi\"])\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "words.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile('FoxInSocks.txt')\n",
    "\n",
    "def hasWhen(line):\n",
    "\treturn 'when' in line\n",
    "\n",
    "whenLines = lines.filter(hasWhen)\n",
    "whenLines.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "lines = sc.textFile('FoxInSocks.txt')\n",
    "\n",
    "whenLines = lines.filter(lambda line: 'when' in line)\n",
    "whenLines.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "numbers = sc.parallelize([1, 2, 3, 4])\n",
    "squared = numbers.map(lambda x: x * x).collect()\n",
    "for num in squared:\n",
    "    print (num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "lines = sc.parallelize([\"hello world\", \"hi\"])\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "words.first() # returns \"hello\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "lines = sc.textFile('FoxInSocks.txt')\n",
    "\n",
    "pairs= lines.map(lambda x: (x.split(\" \")[0], x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pairs.filter(lambda x: len(x[1]) < 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "lines = sc.textFile('FoxInSocks.txt')\n",
    "\n",
    "words = lines.flatMap(lambda x: x.split(\" \"))\n",
    "pairs= words.map(lambda x: (x, 1))\n",
    "pairs.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "lines = sc.parallelize([\"hello world\", \"hi\"])\n",
    "words = lines.map(lambda line: line.split(\" \"))\n",
    "words.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "lines = sc.parallelize([\"hello world\", \"hi\"])\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "words.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "lines = sc.textFile('FoxInSocks.txt')\n",
    "\n",
    "words = lines.flatMap(lambda x: x.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs= words.map(lambda x: (x, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pairs.reduce(lambda x, y: x + y)\n",
    "print(results)\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "numbers = sc.parallelize([1, 2, 3, 4, 5, 6])\n",
    "squared = numbers.map(lambda x: x * x)\n",
    "squared.collect()\n",
    "squared.reduce(lambda x, y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squared.reduceByKey(lambda x, y: x+y)\n",
    "squared.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**More:** Add here below your own examples "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
